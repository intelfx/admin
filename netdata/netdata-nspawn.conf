# netdata configuration
#
# You can download the latest version of this file, using:
#
#  wget -O /etc/netdata/netdata.conf http://localhost:19999/netdata.conf
# or
#  curl -o /etc/netdata/netdata.conf http://localhost:19999/netdata.conf
#
# You can uncomment and change any of the options below.
# The value shown in the commented settings, is the default value.
#

# global netdata configuration

[global]
	#| >>> [global].run as user <<<
	#| datatype: text, default value: netdata
	run as user = netdata

	#| >>> [global].hostname <<<
	#| datatype: text, default value: ...
	# hostname = ...

	#| >>> [global].OOM score <<<
	#| datatype: text, default value: 0
	OOM score = keep

	#| >>> [global].process scheduling policy <<<
	#| datatype: text, default value: batch
	process scheduling policy = keep

	# glibc malloc arena max for plugins = 1
	# glibc malloc arena max for netdata = 1
	# cpu cores = 32
	# libuv worker threads = 192
	# host access prefix = 
	# timezone = Europe/Moscow
	# pthread stack size = 8MiB
	# is ephemeral node = no
	# has unstable connection = no

[db]
	#| >>> [db].update every <<<
	#| datatype: duration (seconds), default value: 1s
	update every = 1s

	#| >>> [db].db <<<
	#| datatype: text, default value: dbengine
	db = dbengine

	#| >>> [db].dbengine page cache size <<<
	#| datatype: size (MiB), default value: 32MiB
	dbengine page cache size = 512MiB

	#| >>> [db].dbengine tier 0 retention size <<<
	#| datatype: size (MiB), default value: 1GiB
	dbengine tier 0 retention size = 10GiB

	#| >>> [db].cleanup obsolete charts after <<<
	#| datatype: duration (seconds), default value: 1h
	cleanup obsolete charts after = 1h

	#| >>> [db].cleanup orphan hosts after <<<
	#| datatype: duration (seconds), default value: 1h
	cleanup orphan hosts after = 1d

	#| >>> [db].cleanup ephemeral hosts after <<<
	#| datatype: duration (seconds), default value: 1d
	cleanup ephemeral hosts after = 1d

	#| >>> [db].enable replication <<<
	#| datatype: yes or no, default value: yes
	enable replication = yes

	#| >>> [db].replication period <<<
	#| datatype: duration (seconds), default value: 1d
	replication period = 1y

	#| >>> [db].replication step <<<
	#| datatype: duration (seconds), default value: 10m
	replication step = 10m

	#| >>> [db].dbengine use direct io <<<
	#| datatype: yes or no, default value: yes
	dbengine use direct io = no

	#| >>> [db].storage tiers <<<
	#| datatype: number (integer), default value: 3
	storage tiers = 3

	#| >>> [db].dbengine tier 1 retention size <<<
	#| datatype: size (MiB), default value: 1GiB
	dbengine tier 1 retention size = 10GiB

	#| >>> [db].dbengine tier 1 update every iterations <<<
	#| datatype: number (integer), default value: 60
	dbengine tier 1 update every iterations = 60

	#| >>> [db].dbengine tier 2 retention size <<<
	#| datatype: size (MiB), default value: 1GiB
	dbengine tier 2 retention size = 10GiB

	#| >>> [db].dbengine tier 2 update every iterations <<<
	#| datatype: number (integer), default value: 60
	dbengine tier 2 update every iterations = 60

	# dbengine page type = gorilla
	# dbengine extent cache size = off
	# dbengine enable journal integrity check = no
	# memory deduplication (ksm) = auto
	# gap when lost iterations above = 1
	# dbengine pages per extent = 109
	# dbengine tier backfill = new
	# dbengine tier 0 retention time = off
	# dbengine tier 1 retention time = off
	# dbengine tier 2 retention time = off
	# replication threads = 1

[directories]
	# config = /etc/netdata
	# stock config = /usr/lib/netdata/conf.d
	# log = /var/log/netdata
	# web = /usr/share/netdata/web
	# cache = /var/cache/netdata
	# lib = /var/lib/netdata
	# lock = /var/lib/netdata/lock
	# cloud.d = /var/lib/netdata/cloud.d
	# plugins = "/usr/lib/netdata/plugins.d" "/etc/netdata/custom-plugins.d"
	# home = /var/cache/netdata
	# registry = /var/lib/netdata/registry
	# stock health config = /usr/lib/netdata/conf.d/health.d
	# health config = /etc/netdata/health.d

[logs]
	#| >>> [logs].debug <<<
	#| datatype: text, default value: /var/log/netdata/debug.log
	debug = journal

	#| >>> [logs].daemon <<<
	#| datatype: text, default value: journal
	daemon = journal

	#| >>> [logs].collector <<<
	#| datatype: text, default value: journal
	collector = journal

	#| >>> [logs].access <<<
	#| datatype: text, default value: /var/log/netdata/access.log
	access = none

	#| >>> [logs].health <<<
	#| datatype: text, default value: journal
	health = journal

	# debug flags = 0x0000000000000000
	# facility = daemon
	# logs flood protection period = 1m
	# logs to trigger flood protection = 1000
	# level = info

[environment variables]
	# PATH = /usr/local/sbin:/usr/local/bin:/usr/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
	# PYTHONPATH = 
	# TZ = :/etc/localtime

[host labels]
	kind = nspawn

[cloud]
	# conversation log = no
	# scope = full
	# query thread count = 6
	# proxy = env

[ml]
	# enabled = auto
	# maximum num samples to train = 21600
	# minimum num samples to train = 900
	# train every = 3h
	# number of models per dimension = 18
	# delete models older than = 7d
	# num samples to diff = 1
	# num samples to smooth = 3
	# num samples to lag = 5
	# random sampling ratio = 0.20000
	# maximum number of k-means iterations = 1000
	# dimension anomaly score threshold = 0.99000
	# host anomaly rate threshold = 1.00000
	# anomaly detection grouping method = average
	# anomaly detection grouping duration = 5m
	# num training threads = 4
	# flush models batch size = 128
	# dimension anomaly rate suppression window = 15m
	# dimension anomaly rate suppression threshold = 450
	# enable statistics charts = no
	# hosts to skip from training = !*
	# charts to skip from training = netdata.*
	# stream anomaly detection charts = yes

[health]
	# silencers file = /var/lib/netdata/health.silencers.json
	# enabled = yes
	# enable stock health configuration = yes
	# use summary for notifications = yes
	# default repeat warning = off
	# default repeat critical = off
	# in memory max health log entries = 1000
	# health log retention = 5d
	# script to execute on alarm = /usr/lib/netdata/plugins.d/alarm-notify.sh
	# enabled alarms = *
	# run at least every = 10s
	# postpone alarms during hibernation for = 1m

[web]
	#| >>> [web].ssl key <<<
	#| datatype: text, default value: /etc/netdata/ssl/key.pem
	# ssl key = /etc/netdata/ssl/key.pem

	#| >>> [web].ssl certificate <<<
	#| datatype: text, default value: /etc/netdata/ssl/cert.pem
	# ssl certificate = /etc/netdata/ssl/cert.pem

	#| >>> [web].bind to <<<
	#| datatype: text, default value: *
	bind to = *:19999=dashboard|registry|badges|management|streaming|netdata.conf^SSL=optional *:19998=streaming^SSL=force unix:/run/netdata/netdata.sock^SSL=optional

	#| >>> [web].ssl skip certificate verification <<<
	#| datatype: yes or no, default value: no
	ssl skip certificate verification = no

	# tls version = 1.3
	# tls ciphers = none
	# ses max tg_des_window = 15
	# des max tg_des_window = 15
	# mode = static-threaded
	# listen backlog = 4096
	# default port = 19999
	# bearer token protection = no
	# disconnect idle clients after = 1m
	# timeout for first request = 1m
	# accept a streaming request every = off
	# respect do not track policy = no
	# x-frame-options response header = 
	# allow connections from = localhost *
	# allow connections by dns = heuristic
	# allow dashboard from = localhost *
	# allow dashboard by dns = heuristic
	# allow badges from = *
	# allow badges by dns = heuristic
	# allow streaming from = *
	# allow streaming by dns = heuristic
	# allow netdata.conf from = localhost fd* 10.* 192.168.* 172.16.* 172.17.* 172.18.* 172.19.* 172.20.* 172.21.* 172.22.* 172.23.* 172.24.* 172.25.* 172.26.* 172.27.* 172.28.* 172.29.* 172.30.* 172.31.* UNKNOWN
	# allow netdata.conf by dns = no
	# allow management from = localhost
	# allow management by dns = heuristic
	# enable gzip compression = yes
	# gzip compression strategy = default
	# gzip compression level = 3
	# web server threads = 6
	# web server max sockets = 131072
	# custom dashboard_info.js = 

[registry]
	# enabled = no
	# netdata unique id file = /var/lib/netdata/registry/netdata.public.unique.id
	# registry db file = /var/lib/netdata/registry/registry.db
	# registry log file = /var/lib/netdata/registry/registry-log.db
	# registry save db every new entries = 1000000
	# registry expire idle persons = 1y
	# registry domain = 
	# registry to announce = https://registry.my-netdata.io
	# registry hostname = ...
	# verify browser cookies support = yes
	# enable cookies SameSite and Secure = yes
	# max URL length = 1024
	# max URL name length = 50
	# use mmap = no
	# netdata management api key file = /var/lib/netdata/netdata.api.key
	# allow from = *
	# allow by dns = heuristic

[global statistics]
	# update every = 1s

[plugins]
	#| >>> [plugins].netdata monitoring <<<
	#| datatype: yes or no, default value: yes
	netdata monitoring = yes

	#| >>> [plugins].netdata monitoring extended <<<
	#| datatype: yes or no, default value: no
	netdata monitoring extended = yes

	#| >>> [plugins].diskspace <<<
	#| datatype: yes or no, default value: yes
	diskspace = no

	#| >>> [plugins].debugfs <<<
	#| datatype: yes or no, default value: yes
	debugfs = no

	#| >>> [plugins].freeipmi <<<
	#| datatype: yes or no, default value: yes
	freeipmi = no

	# idlejitter = yes
	# profile = no
	# tc = yes
	# proc = yes
	# cgroups = yes
	# timex = yes
	# enable running new plugins = yes
	# check for new plugins every = 60
	# slabinfo = no
	# apps = yes
	# statsd = yes
	# charts.d = yes
	# cups = yes
	# ebpf = yes
	# go.d = yes
	# ioping = yes
	# network-viewer = yes
	# nfacct = yes
	# perf = yes
	# python.d = yes
	# systemd-journal = yes

[statsd]
	# update every (flushInterval) = 1s
	# udp messages to process at once = 10
	# create private charts for metrics matching = *
	# max private charts hard limit = 1000
	# set charts as obsolete after = off
	# decimal detail = 1000
	# disconnect idle tcp clients after = 10m
	# private charts hidden = no
	# histograms and timers percentile (percentThreshold) = 95.00000
	# dictionaries max unique dimensions = 200
	# add dimension for number of events received = no
	# gaps on gauges (deleteGauges) = no
	# gaps on counters (deleteCounters) = no
	# gaps on meters (deleteMeters) = no
	# gaps on sets (deleteSets) = no
	# gaps on histograms (deleteHistograms) = no
	# gaps on timers (deleteTimers) = no
	# gaps on dictionaries (deleteDictionaries) = no
	# statsd server max TCP sockets = 131072
	# listen backlog = 4096
	# default port = 8125
	# bind to = udp:localhost tcp:localhost

[plugin:timex]
	# update every = 10s
	# clock synchronization state = yes
	# time offset = yes

[plugin:idlejitter]
	# loop time = 20ms

[plugin:apps]
	# update every = 1s
	# command options = 

[plugin:charts.d]
	# update every = 1s
	# command options = 

[plugin:cups]
	# update every = 1s
	# command options = 

[plugin:go.d]
	# update every = 1s
	# command options = 

[plugin:tc]
	# script to run to get tc values = /usr/lib/netdata/plugins.d/tc-qos-helper.sh

[plugin:ioping]
	# update every = 1s
	# command options = 

[plugin:nfacct]
	# update every = 1s
	# command options = 

[plugin:cgroups]
	# update every = 1s
	# check for new cgroups every = 10s
	# use unified cgroups = auto
	# max cgroups to allow = 1000
	# max cgroups depth to monitor = 0
	# enable by default cgroups matching =  !*/init.scope  !/system.slice/run-*.scope  *user.slice/docker-* !*user.slice* *.scope  /machine.slice/*.service  */kubepods/pod*/*  */kubepods/*/pod*/*  */*-kubepods-pod*/*  */*-kubepods-*-pod*/*  !*kubepods* !*kubelet*  !*/vcpu*  !*/emulator  !*.mount  !*.partition  !*.service  !*.service/udev  !*.socket  !*.slice  !*.swap  !*.user  !/  !/docker  !*/libvirt  !/lxc  !/lxc/*/*  !/lxc.monitor*  !/lxc.pivot  !/lxc.payload  !*lxcfs.service/.control !/machine  !/qemu  !/system  !/systemd  !/user  * 
	# enable by default cgroups names matching =  * 
	# search for cgroups in subpaths matching =  !*/init.scope  !*-qemu  !*.libvirt-qemu  !/init.scope  !/system  !/systemd  !/user  !/lxc/*/*  !/lxc.monitor  !/lxc.payload/*/*  !/lxc.payload.*  * 
	# script to get cgroup names = /usr/lib/netdata/plugins.d/cgroup-name.sh
	# script to get cgroup network interfaces = /usr/lib/netdata/plugins.d/cgroup-network
	# run script to rename cgroups matching =  !/  !*.mount  !*.socket  !*.partition  /machine.slice/*.service  !*.service  !*.slice  !*.swap  !*.user  !init.scope  !*.scope/vcpu*  !*.scope/emulator  *.scope  *docker*  *lxc*  *qemu*  */kubepods/pod*/*  */kubepods/*/pod*/*  */*-kubepods-pod*/*  */*-kubepods-*-pod*/*  !*kubepods* !*kubelet*  *.libvirt-qemu  * 
	# cgroups to match as systemd services =  !/system.slice/*/*.service  /system.slice/*.service 

[plugin:perf]
	# update every = 1s
	# command options = 

[plugin:python.d]
	# update every = 1s
	# command options = 

[plugin:proc]
	#| >>> [plugin:proc]./proc/pagetypeinfo <<<
	#| datatype: yes or no, default value: no
	/proc/pagetypeinfo = no

	#| >>> [plugin:proc]./proc/stat <<<
	#| datatype: yes or no, default value: yes
	/proc/stat = no

	#| >>> [plugin:proc]./proc/uptime <<<
	#| datatype: yes or no, default value: yes
	/proc/uptime = no

	#| >>> [plugin:proc]./proc/loadavg <<<
	#| datatype: yes or no, default value: yes
	/proc/loadavg = no

	#| >>> [plugin:proc]./proc/sys/fs/file-nr <<<
	#| datatype: yes or no, default value: yes
	/proc/sys/fs/file-nr = no

	#| >>> [plugin:proc]./proc/sys/kernel/random/entropy_avail <<<
	#| datatype: yes or no, default value: yes
	/proc/sys/kernel/random/entropy_avail = no

	#| >>> [plugin:proc]./proc/interrupts <<<
	#| datatype: yes or no, default value: yes
	/proc/interrupts = no

	#| >>> [plugin:proc]./proc/softirqs <<<
	#| datatype: yes or no, default value: yes
	/proc/softirqs = no

	#| >>> [plugin:proc]./proc/vmstat <<<
	#| datatype: yes or no, default value: yes
	/proc/vmstat = no

	#| >>> [plugin:proc]./proc/meminfo <<<
	#| datatype: yes or no, default value: yes
	/proc/meminfo = no

	#| >>> [plugin:proc]./sys/kernel/mm/ksm <<<
	#| datatype: yes or no, default value: yes
	/sys/kernel/mm/ksm = no

	#| >>> [plugin:proc]./sys/block/zram <<<
	#| datatype: yes or no, default value: yes
	/sys/block/zram = no

	#| >>> [plugin:proc]./sys/devices/system/edac/mc <<<
	#| datatype: yes or no, default value: yes
	/sys/devices/system/edac/mc = no

	#| >>> [plugin:proc]./sys/devices/pci/aer <<<
	#| datatype: yes or no, default value: yes
	/sys/devices/pci/aer = no

	#| >>> [plugin:proc]./sys/devices/system/node <<<
	#| datatype: yes or no, default value: yes
	/sys/devices/system/node = no

	#| >>> [plugin:proc]./proc/net/wireless <<<
	#| datatype: yes or no, default value: yes
	/proc/net/wireless = no

	#| >>> [plugin:proc]./proc/net/softnet_stat <<<
	#| datatype: yes or no, default value: yes
	/proc/net/softnet_stat = no

	#| >>> [plugin:proc]./sys/class/infiniband <<<
	#| datatype: yes or no, default value: yes
	/sys/class/infiniband = no

	#| >>> [plugin:proc]./proc/diskstats <<<
	#| datatype: yes or no, default value: yes
	/proc/diskstats = no

	#| >>> [plugin:proc]./proc/mdstat <<<
	#| datatype: yes or no, default value: yes
	/proc/mdstat = no

	#| >>> [plugin:proc]./proc/spl/kstat/zfs/arcstats <<<
	#| datatype: yes or no, default value: yes
	/proc/spl/kstat/zfs/arcstats = no

	#| >>> [plugin:proc]./sys/fs/btrfs <<<
	#| datatype: yes or no, default value: yes
	/sys/fs/btrfs = no

	#| >>> [plugin:proc]./sys/class/power_supply <<<
	#| datatype: yes or no, default value: yes
	/sys/class/power_supply = no

	#| >>> [plugin:proc]./sys/class/drm <<<
	#| datatype: yes or no, default value: yes
	/sys/class/drm = no

	# /proc/net/dev = yes
	# /proc/pressure = yes
	# /proc/net/sockstat = yes
	# /proc/net/sockstat6 = yes
	# /proc/net/netstat = yes
	# /proc/net/sctp/snmp = yes
	# /proc/net/ip_vs/stats = yes
	# /proc/net/stat/conntrack = yes
	# /proc/net/stat/synproxy = yes
	# /proc/net/rpc/nfsd = yes
	# /proc/net/rpc/nfs = yes
	# ipc = yes

[plugin:systemd-journal]
	# update every = 1s
	# command options = 

[plugin:proc:/proc/net/dev]
	# compressed packets for all interfaces = no
	# disable by default interfaces matching = lo fireqos* *-ifb fwpr* fwbr* fwln* ifb4*

[plugin:proc:/proc/pressure]
	# base path of pressure metrics = /proc/pressure
	# enable cpu some pressure = yes
	# enable cpu full pressure = no
	# enable memory some pressure = yes
	# enable memory full pressure = yes
	# enable io some pressure = yes
	# enable io full pressure = yes
	# enable irq some pressure = no
	# enable irq full pressure = yes

[plugin:proc:/proc/net/sockstat]
	# ipv4 sockets = auto
	# ipv4 TCP sockets = auto
	# ipv4 TCP memory = auto
	# ipv4 UDP sockets = auto
	# ipv4 UDP memory = auto
	# ipv4 UDPLITE sockets = auto
	# ipv4 RAW sockets = auto
	# ipv4 FRAG sockets = auto
	# ipv4 FRAG memory = auto
	# update constants every = 1m
	# filename to monitor = /proc/net/sockstat

[plugin:proc:/proc/net/sockstat6]
	# ipv6 TCP sockets = auto
	# ipv6 UDP sockets = auto
	# ipv6 UDPLITE sockets = auto
	# ipv6 RAW sockets = auto
	# ipv6 FRAG sockets = auto
	# filename to monitor = /proc/net/sockstat6

[plugin:proc:/proc/net/netstat]
	# bandwidth = auto
	# input errors = auto
	# multicast bandwidth = auto
	# broadcast bandwidth = auto
	# multicast packets = auto
	# broadcast packets = auto
	# ECN packets = auto
	# TCP reorders = auto
	# TCP SYN cookies = auto
	# TCP out-of-order queue = auto
	# TCP connection aborts = auto
	# TCP memory pressures = auto
	# TCP SYN queue = auto
	# TCP accept queue = auto
	# filename to monitor = /proc/net/netstat

[plugin:proc:/proc/net/snmp]
	# ipv4 packets = auto
	# ipv4 fragments sent = auto
	# ipv4 fragments assembly = auto
	# ipv4 errors = auto
	# ipv4 TCP connections = auto
	# ipv4 TCP packets = auto
	# ipv4 TCP errors = auto
	# ipv4 TCP opens = auto
	# ipv4 TCP handshake issues = auto
	# ipv4 UDP packets = auto
	# ipv4 UDP errors = auto
	# ipv4 ICMP packets = auto
	# ipv4 ICMP messages = auto
	# ipv4 UDPLite packets = auto
	# filename to monitor = /proc/net/snmp

[plugin:proc:/proc/net/snmp6]
	# ipv6 packets = auto
	# ipv6 fragments sent = auto
	# ipv6 fragments assembly = auto
	# ipv6 errors = auto
	# ipv6 UDP packets = auto
	# ipv6 UDP errors = auto
	# ipv6 UDPlite packets = auto
	# ipv6 UDPlite errors = auto
	# bandwidth = auto
	# multicast bandwidth = auto
	# broadcast bandwidth = auto
	# multicast packets = auto
	# icmp = auto
	# icmp redirects = auto
	# icmp errors = auto
	# icmp echos = auto
	# icmp group membership = auto
	# icmp router = auto
	# icmp neighbor = auto
	# icmp mldv2 = auto
	# icmp types = auto
	# ect = auto
	# filename to monitor = /proc/net/snmp6

[plugin:proc:/proc/net/sctp/snmp]
	# established associations = auto
	# association transitions = auto
	# fragmentation = auto
	# packets = auto
	# packet errors = auto
	# chunk types = auto
	# filename to monitor = /proc/net/sctp/snmp

[plugin:proc:/proc/net/ip_vs_stats]
	# IPVS bandwidth = yes
	# IPVS connections = yes
	# IPVS packets = yes
	# filename to monitor = /proc/net/ip_vs_stats

[plugin:proc:/proc/net/stat/nf_conntrack]
	# filename to monitor = /proc/net/stat/nf_conntrack
	# netfilter new connections = yes
	# netfilter connection changes = yes
	# netfilter connection expectations = yes
	# netfilter connection searches = yes
	# netfilter errors = yes
	# netfilter connections = yes

[plugin:proc:/proc/sys/net/netfilter/nf_conntrack_max]
	# filename to monitor = /proc/sys/net/netfilter/nf_conntrack_max
	# read every seconds = 10

[plugin:proc:/proc/net/stat/synproxy]
	# SYNPROXY cookies = auto
	# SYNPROXY SYN received = auto
	# SYNPROXY connections reopened = auto
	# filename to monitor = /proc/net/stat/synproxy

[plugin:proc:/proc/net/rpc/nfsd]
	# filename to monitor = /proc/net/rpc/nfsd
	# read cache = yes
	# file handles = yes
	# I/O = yes
	# threads = yes
	# network = yes
	# rpc = yes
	# NFS v2 procedures = yes
	# NFS v3 procedures = yes
	# NFS v4 procedures = yes
	# NFS v4 operations = yes

[plugin:proc:/proc/net/rpc/nfs]
	# filename to monitor = /proc/net/rpc/nfs

[plugin:proc:ipc]
	# message queues = yes
	# semaphore totals = yes
	# shared memory totals = yes
	# msg filename to monitor = /proc/sysvipc/msg
	# shm filename to monitor = /proc/sysvipc/shm
	# max dimensions in memory allowed = 50

[plugin:ebpf]
	# update every = 1s
	# command options = 

[plugin:network-viewer]
	# update every = 1s
	# command options = 
